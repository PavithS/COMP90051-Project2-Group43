{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d2c349-07e9-4280-8ed7-8bf15c995784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d96351-d006-4a93-9a37-930a2dd7d951",
   "metadata": {},
   "source": [
    "Training Data: 'combined_text' concatinates title and abstract arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cb3540-14ff-4d6e-8a3b-c8def5b7739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.read_csv('./Data/train.csv')\n",
    "traindf['abstract'] = traindf['abstract'].apply(lambda abstract: abstract.strip('[]').split(', '))\n",
    "traindf['title'] = traindf['title'].apply(lambda title: title.strip('[]').split(', '))\n",
    "traindf['authors'] = traindf['authors'].apply(lambda authors: authors.strip('[]').split(', '))\n",
    "traindf['combined_text'] = traindf.title+traindf.abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214037d5-5fba-4cb6-a07a-31c41e51b240",
   "metadata": {},
   "source": [
    "Word2vec embedding: All encoded words in training data (abstract and title) converted to an 600 dimensinal embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b50680c-c940-496f-bcd9-b839a2d5ce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da810e4-7110-435a-962b-ffaf993e2918",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSG = gensim.models.Word2Vec(traindf['combined_text'], min_count = 1, vector_size = 600, window = 5, sg = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03d99a3-2e56-4008-9016-fe0a43a1dcdd",
   "metadata": {},
   "source": [
    "Training labels: For each article an array of 0 or 1 indicating whether author $i \\in {0,...,99}$ is a target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6106612b-901f-4f3b-806f-a452bcda03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def targetLabels(authorData):\n",
    "    labels = np.zeros((len(authorData), 100))\n",
    "    for i, article in enumerate(authorData):\n",
    "        for j, author in enumerate(article):\n",
    "            author = int(author)\n",
    "            if author < 100:\n",
    "                labels[i][author] = 1\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d61385-b1a8-450a-acc5-00802a26b5a6",
   "metadata": {},
   "source": [
    "Functions for converting coauthors (non-prolific) for an article to vectors (i-th element of j-th vector is 1 if i-th coauthor contributed to j-th article), and for converting words in each article to the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc8c838-09e6-4002-9a29-960cfb05b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coauthorsToVectors(authorData, n_authors):\n",
    "    coauths = np.zeros((len(authorData), n_authors))\n",
    "    for i, article in enumerate(authorData):\n",
    "        for j, author in enumerate(article):\n",
    "            author = int(author)\n",
    "            if author >= 100:\n",
    "                coauths[i][author] = 1\n",
    "    return coauths\n",
    "\n",
    "def embedLine(embedding, line):\n",
    "    arr = []\n",
    "    for li, word in enumerate(line):\n",
    "        try:\n",
    "            arr.append(embedding[word])\n",
    "        except:\n",
    "            continue\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1c183e-79f7-40c3-8d49-e41829bb5768",
   "metadata": {},
   "source": [
    "GRU with attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad6cb7b-3b1e-4690-b7f8-452a7ce8e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionGRUClassifier(nn.Module):\n",
    "    def __init__(self, embed_size, n_authors, hidden_size, text_output_size, coauth_output_size, output_size):\n",
    "        super(AttentionGRUClassifier, self).__init__()\n",
    "        self.gru = nn.GRU(embed_size, hidden_size)\n",
    "        self.text_layer = nn.Linear(hidden_size, text_output_size)\n",
    "        self.att = nn.Linear(hidden_size, 1)\n",
    "        self.coauth_layer = nn.Linear(n_authors, coauth_output_size)\n",
    "        self.final = nn.Linear(text_output_size+coauth_output_size, output_size)\n",
    "        \n",
    "    def forward(self, input_words, input_coauthors):\n",
    "        # Process text (abstract/title) into RNN hidden state sequence\n",
    "        states, _ = self.gru(input_words)\n",
    "        # Attention scores for each hidden state\n",
    "        att_scores = self.att(states)\n",
    "        # Rescale attention with softmax\n",
    "        alpha = F.log_softmax(att_scores, dim=0)\n",
    "        # Compute weighted sum of hidden states\n",
    "        c = torch.sum(torch.mul(states, alpha), dim=0)\n",
    "        # Compute output, and apply log-softmax\n",
    "        text_output = F.relu(self.text_layer(c.view(1, -1)))\n",
    "        coauth_output = F.relu(self.coauth_layer(input_coauthors.view(1,-1)))\n",
    "        combined_output = torch.cat((text_output, coauth_output), dim=1).view(1,-1)\n",
    "        output = self.final(combined_output)\n",
    "        output = F.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841c9316-5eb8-4dcc-847d-9dbaf27958cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = np.array([embedLine(modelSG.wv, traindf.combined_text[i]) for i in range(traindf.shape[0])], dtype=object)\n",
    "train_labels = torch.tensor(targetLabels(traindf.authors)).view(-1,1,100).to(torch.float32)\n",
    "train_coauthors = torch.tensor(coauthorsTo1Hot(traindf.authors, 21246)).view(-1,1,21246).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d50e21-0260-4ad6-a6b8-20c822223709",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed_size = 600\n",
    "hidden_size = 200\n",
    "text_output_size = 100\n",
    "n_authors = 21246\n",
    "coauth_output_size = 100\n",
    "output_size = 100\n",
    "\n",
    "model = AttentionGRUClassifier(embed_size, n_authors, hidden_size, text_output_size, coauth_output_size, output_size)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "n_epochs = 4\n",
    "n_samples = traindf.shape[0]\n",
    "print_every = 5000\n",
    "plot_every = 500\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "current_loss = 0\n",
    "iter_count = 0\n",
    "all_epoch_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "start = time.time()\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0\n",
    "    for sample_id in random.sample(range(n_samples), n_samples):\n",
    "        iter_count += 1\n",
    "        text_tensor = torch.tensor(train_text[sample_id]).view(-1, 1, 600)\n",
    "        label_tensor = train_labels[sample_id]\n",
    "        coauthor_tensor = train_coauthors[sample_id]\n",
    "\n",
    "        model.zero_grad()\n",
    "        output = model.forward(text_tensor, coauthor_tensor)\n",
    "        output = torch.squeeze(output, 1) # remove redundant dimension\n",
    "        loss = criterion(output, label_tensor)\n",
    "        current_loss += loss.item()\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if iter_count % print_every == 0:\n",
    "            print('epoch: %d, total iterations: %d, progress: %d%% (%s), loss: %.4f' % (epoch, iter_count, iter_count / (n_epochs*n_samples) * 100, timeSince(start), loss))\n",
    "            \n",
    "        if iter_count % plot_every == 0:\n",
    "            all_losses.append(current_loss / plot_every)\n",
    "            current_loss = 0\n",
    "    all_epoch_losses.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ab48f-e40e-4547-83a2-da56ca253106",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(plot_every,n_epochs*n_samples,plot_every),all_losses, label='gru+attention')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss (Train)')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(0, n_epochs, 1),all_epoch_losses, label='gru+attention')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (Train)')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
